# Deep Q-Network (DQN) for Chess ‚Äî Theory and Implementation Map

This note explains how canonical DQN works in theory, how we map it to the chess setting in this repo, and where the current implementation deviates from the ideal. It‚Äôs written to help you make informed decisions and to revisit details over time.

## 1) Conceptual Overview

Goal: Learn an action-value function Q(s, a) that estimates the expected discounted return when taking action a in state s and following the current policy thereafter.

Key components in ‚Äúvanilla‚Äù DQN:
- Neural network QŒ∏(s, a) approximates Q-values for all actions given a state.
- Target network QŒ∏¬Ø(s, a) is a periodically updated copy of QŒ∏ to stabilize targets.
- Experience replay buffer stores (s, a, r, s‚Ä≤, done) tuples to decorrelate updates.
- Bellman target: y = r if done else r + Œ≥ max_{a‚Ä≤} QŒ∏¬Ø(s‚Ä≤, a‚Ä≤)
- Loss: mean-squared-error between QŒ∏(s, a) and y (or Huber loss for robustness)
- Exploration: typically Œµ-gredy on Q-values.

## 2) Chess-specific mapping

- State space: 839 features per state (12 piece planes + side to move + castling rights + en passant one‚Äëhot 64 + clocks). Aligned with chess‚Äëengine FeatureEncoding.
- Action space: 4096 indices (fromSquare √ó toSquare), promotions handled via matching to legal moves
- Sparse/terminal rewards: +1/-1/0 on win/loss/draw; optional position-based shaping; optional step-limit penalty
- Valid action masking is crucial: Most of the 4096 actions are illegal in a given chess position; max over illegal actions corrupts targets.

## 3) Implementation Map (this repo)

Files to know (main pathnames abbreviated):
- rl-framework/RLAlgorithms.kt: DQNAlgorithm (Q-learning, target updates, replay), exploration strategies, metrics types
- integration/RealChessAgentFactory.kt: Builds real DQN agent (FeedforwardNetwork + Adam + Huber + replay), wires target frequency
- integration/ChessTrainingPipeline.kt: Wires next-state masking provider into the agent (agent.setNextActionProvider)
- integration/AdvancedSelfPlayTrainingPipeline.kt: Orchestrates self-play ‚Üí experience processing ‚Üí validated batch training ‚Üí evaluation, checkpoints, rollback
- integration/SelfPlaySystem.kt: (Now) concurrent self-play with per-game threads; produces EnhancedExperience
- integration/ChessEnvironment.kt: Encodes state, validates and executes moves, computes rewards, exposes valid actions
- nn-package/NeuralNetwork.kt: FeedforwardNetwork (layers, training loop, Adam, Huber), model save/load (JVM)

### 3.1 Q-network and target network
- Construction: RealChessAgentFactory.createRealDQNAgent builds two identical FeedforwardNetwork instances for Q and target.
- Optimizer/loss: Adam + HuberLoss(Œ¥=1.0) by default
- Synchronization cadence: targetUpdateFrequency updates (default 100, profile/CLI overridable)
- Copy mechanism: DQNAlgorithm.updateTargetNetwork() via SynchronizableNetwork.copyWeightsTo

### 3.2 Experience replay
- Experience buffers:
  - CircularExperienceBuffer (unseeded) and SeededCircularExperienceBuffer
  - DQNAlgorithm.updatePolicy(experiences) appends new tuples and samples a batch when size ‚â• batchSize
- Bellman targets:
  - computeQTargets(batch): uses target network and (importantly) valid-action masking when provided (more below)

### 3.3 Action selection and exploration
- Exploration strategy: EpsilonGreedyStrategy (or seeded variant)
- selectAction: choose argmax Q(s, a) over valid actions with Œµ-greedy exploration

### 3.4 Valid-action masking (critical for chess)
- Where it‚Äôs set: ChessTrainingPipeline.init calls agent.setNextActionProvider(environment::getValidActions)
- DQNAlgorithm.computeQTargets:
  - If provider is set, it queries valid next actions for each s‚Ä≤ and computes max only over those actions.
  - One-time logs confirm masking is active:
    - ‚Äúüß© DQN next-state action masking enabled (provider set)‚Äù
    - ‚Äúüß© DQN masking applied: valid next actions for sample=N‚Äù
- Why it matters: prevents illegal-move Q-values from inflating targets and stalling learning.
- Caveat: environment.getValidActions(state) currently ignores the passed DoubleArray and uses the internal board. This is fine for on-policy calls in the same env, but for replay we want true per-state masking. See ¬ß6 for improvements.

### 3.5 Training loop in advanced pipeline
- Self-play (concurrent): SelfPlaySystem generates games between main and opponent agents; produces EnhancedExperience (with metadata and chess metrics)
- Experience processing: AdvancedExperienceManager filters/assesses quality; produces training batches
- Validated batch training: AdvancedSelfPlayTrainingPipeline.performValidatedBatchTraining
  - Batches are converted to basic experiences and passed to agent.trainBatch (which calls DQNAlgorithm.updatePolicy)
  - Metrics reported: loss, gradient norm (proxy), entropy; optional validation checks
- Target sync visibility: ‚ÄúüîÅ DQN target network synchronized at update=‚Ä¶ (freq=‚Ä¶)‚Äù shows up after the configured number of policy updates

### 3.6 Checkpointing and retention
- CheckpointManager creates checkpoints regularly and at end; model saved via FeedforwardNetwork.save
- Retention policy: keep best, last N, and optionally every N-th; cleanup metadata-level by default (JVM file deletion could be added)

## 4) Theoretical ‚Äúideal‚Äù vs current implementation

| Aspect | Ideal DQN | Current status |
| --- | --- | --- |
| Q-network | MLP with stable optimizer & loss | FeedforwardNetwork + Adam + Huber (OK) |
| Target network | Periodically synced | Implemented, freq configurable (OK) |
| Replay | IID batches from large buffer | Circular buffers (OK) |
| Masking | True per-state valid-action masking for s‚Ä≤ | Provider wired; env ignores passed state (needs improvement) |
| Exploration | Œµ-greedy | Implemented; warmup supported (OK) |
| Rewards | Terminal + optional shaping | Configurable; step-limit penalty supported (OK) |
| Concurrency | Parallel self-play | Thread pool per batch (JVM) (OK) |
| Metrics | True loss/entropy/grad stats | Reported from PolicyUpdateResult (OK) |
| Serialization | Save/load model | Implemented on JVM (OK) |

## 5) Practical guidance (DQN for chess)

- Keep masking enabled (default) and verify logs once per run.
- Use shorter games (e.g., 80‚Äì100 max steps) and a small negative step-limit penalty to avoid flat random games.
- Consider reward shaping early (enablePositionRewards=true) to provide signal when decisive results are rare.
- Target update frequency 50‚Äì200 is typical; use 20 for quick debug.
- Check loss, entropy, gradient-norm and self-play win/draw rates: expect modest trends, not dramatic changes in a few cycles.
- Concurrency: set maxConcurrentGames near your core count; we synchronize agent calls internally to ensure thread safety.

## 5.1) ‚ÄúAdvanced‚Äù Training (what the CLI actually does)

Command example
```
./gradlew :integration:runCli -Dargs="--train-advanced --cycles 3 --profile dqn_debug_masking --seed 12345"
```

What happens (one end‚Äëto‚Äëend cycle)
- Phase 1 ‚Äî Self‚Äëplay generation
  - Runs N concurrent self‚Äëplay games (thread pool) until the requested total is reached.
  - Produces EnhancedExperience per move with chess metadata and a base quality score.
  - Progress lines show draw/win rates, average game length, and experiences collected.
- Phase 2 ‚Äî Experience processing
  - Quality assessment and filtering; buffers updated (main/high‚Äëquality/recent) with simple retention rules.
- Phase 3 ‚Äî Batch training with validation
  - Samples experiences into batches and calls `agent.trainBatch` (DQNAlgorithm.updatePolicy) per batch.
  - Prints per‚Äëbatch metrics from the algorithm result (loss, policy entropy, gradient norm; optional EMA and extras).
  - Periodically prints target sync: `üîÅ DQN target network synchronized at update=‚Ä¶ (freq=‚Ä¶)`.
- Phase 4 ‚Äî Evaluation (internal)
  - Plays a small number of evaluation games to estimate reward/win/draw/loss and compute a composite performance score.
- Checkpointing
  - Creates an initial checkpoint (v0), regular checkpoints every `checkpointInterval` cycles, and a final checkpoint.
  - Optional retention cleanup at the end (keep best + last K + every Nth) per config/profile.

Why you sometimes see ‚ÄúCycle 4/3‚Äù
- The advanced pipeline treats `--cycles K` as K + 1 iterations with an initial warmup cycle in some configurations. That‚Äôs why a 3‚Äëcycle run can display ‚ÄúAdvanced Training Cycle 4/3‚Äù.

Notes on flags/profiles
- Profiles (profiles.yaml) set defaults for: algorithm, max steps, concurrency, reward shaping, warmups, checkpoint cadence, retention, target sync frequency, and draw reporting for step‚Äëlimit.
- CLI flags override profile keys: `--max-steps`, `--concurrency`, `--target-update`, `--checkpoint-interval`, retention flags, etc.
- Masking verification: once per run you should see
  - `üß© DQN next-state action masking enabled (provider set)`
  - `üß© DQN masking applied: valid next actions for sample=N`

About `--double-dqn`
- Double‚ÄëDQN changes the target to use the online net to select the next action and the target net to evaluate it. Supported here; enable via profile (e.g., `dqn_debug_masking`, `dqn_imitation_bootstrap`) or `--double-dqn`.

## 5.2) How to verify that something was learned

From logs (first few cycles)
- Target syncs: You should see `üîÅ ‚Ä¶ synchronized ‚Ä¶` at the configured frequency (e.g., every 50 updates). This confirms updates are flowing.
- Entropy < 8.29: Policy entropy near ln(4096) ‚âà 8.29 means uniform. Seeing values lower than that (e.g., ~3.0) indicates non‚Äëuniform preferences emerging. Beware: too low too fast can signal collapse; modest reduction is good.
- Loss variability: Exact scale depends on network/loss; look for non‚Äëflat behavior and mild downward drift over cycles.
- Self‚Äëplay stats: With short episodes and shaping, expect many step‚Äëlimit draws early. As the policy improves, evaluate vs the baseline heuristic to see improvements independent of self‚Äëplay reporting.

Cross‚Äëcycle checks (recommended)
- Baseline evaluation: Run `--eval-baseline --games 20 --load-best` after a few cycles. Compare win/draw/loss to earlier runs.
- Experience buffer quality: Use the analyzer to check reward variance > 0, terminal ratio > 0, and reasonable action diversity.
- Checkpoint diffs: Ensure new checkpoints are being created and the ‚Äúbest model‚Äù version eventually advances beyond 0.

If you see 100% draws
- With `maxStepsPerGame=80..100` and `treatStepLimitAsDraw=true`, many early games will report as draws by step limit. That‚Äôs OK ‚Äî verify that
  - Masking logs appear,
  - Batch metrics vary,
  - Baseline evaluation slowly improves.

## 5.3) Metric glossary (as printed)
- loss: The batch loss (Huber/MSE proxy for TD error) aggregated over samples.
- gradient norm: A proxy norm at the network output (or true norm when available) for stability checks.
- policy entropy: Entropy computed from softmax over Q-values or policy logits; lower implies more certainty.
- td (if printed): An estimate of TD error magnitude (implementation‚Äëdependent; not always printed).

## 6) Known gaps and next steps

1) True per-state masking for replay
- Issue: environment.getValidActions(state: DoubleArray) uses internal board; for replayed (s‚Ä≤) it should compute from the vector.
- Options:
  - Decode function: state ‚Üí board (inverse of encoder) and compute legal moves on the synthetic board.
  - Per-experience mask: store valid-action masks alongside experiences and extend DQNAlgorithm to consume masks in targets.

2) Metrics & monitoring
- Replace simulated performance monitoring with real timings around: NN forward/backward/train, replay sampling, move generation.

3) Compression
- Checkpoints use .json.gz extension but content is JSON. Add actual gzip for JVM saves/loads if desired.

4) Opponent strategy
- Profiles currently copy main weights or use simple strategies. More varied opponents can speed training robustness.

5) Double‚ÄëDQN
- Optionally add a Double‚ÄëDQN mode (profile/flag) that selects the argmax action with the online Q net and evaluates it with the target net in the target computation.

## 7) Pseudocode (reference)

Canonical DQN (with masking and target net):
```
Initialize QŒ∏ and target QŒ∏¬Ø; replay buffer D
for each step do
  Observe s, select a=Œµ-greedy(QŒ∏(s,¬∑)), execute, get r, s‚Ä≤, done
  D.add(s,a,r,s‚Ä≤,done)
  if |D| ‚â• batchSize:
    B ‚Üê sample minibatch from D
    for (s,a,r,s‚Ä≤,done) in B:
      if done: y ‚Üê r
      else:
        A_valid ‚Üê valid_actions(s‚Ä≤)
        y ‚Üê r + Œ≥ max_{a‚Ä≤‚ààA_valid} QŒ∏¬Ø(s‚Ä≤,a‚Ä≤)
      L ‚Üê Huber(QŒ∏(s,a), y)  // or MSE
    Œ∏ ‚Üê Œ∏ - Œ± ‚àáŒ∏ L
  Every N updates: QŒ∏¬Ø ‚Üê QŒ∏  // target sync
```

In our pipeline:
- SelfPlaySystem generates Batches of EnhancedExperience via parallel games
- AdvancedExperienceManager processes/filters into minibatches
- performValidatedBatchTraining calls agent.trainBatch (‚Üí DQNAlgorithm.updatePolicy)
- Masking provider gives valid actions for s‚Ä≤ (with the caveat described above)
- Target sync prints a log when triggered

## 8) Quick glossary
- Q-network: Approximator for action-values Q(s,a)
- Target network: Fixed-lag copy of Q-network for stable targets
- Replay buffer: Storage for experience tuples, sampled IID for updates
- Masking: Restrict backups to valid actions, crucial in large, constrained action spaces
- Œµ-greedy: Choose best action w.p. 1-Œµ else random valid action
- Huber loss: Robust loss less sensitive to outliers than MSE

---
If you want, I can prototype ‚Äúper-state‚Äù masking via masks embedded in experiences or build the inverse encoder to decode state arrays to ChessBoard for exact legality checks in replay.
## Functional Summary (step-by-step)

- Command: `./gradlew :integration:runCli -Dargs="--train-advanced --cycles 3 --profile dqn_debug_masking --seed 12345"`
- Effects by phase:
  - Phase 1: Self-play generation
    - Runs 20 games with 8 concurrent workers (thread pool).
    - Collects EnhancedExperience tuples; reports progress (win/draw, avg length).
  - Phase 2: Experience processing
    - Computes quality, filters, fills buffers.
  - Phase 3: Batch training with validation
    - Samples experiences into batches; calls `DQNAlgorithm.updatePolicy` (via `agent.trainBatch`) per batch.
    - Prints per-batch metrics: loss, gradient norm (and EMA), policy entropy (and EMA).
    - Prints target sync when frequency is hit: `üîÅ DQN target network synchronized at update=40 (freq=20)`.
    - Masking logs (printed once early) confirm valid-action masking for next-state targets:
      - `üß© DQN next-state action masking enabled (provider set)`
      - `üß© DQN masking applied: valid next actions for sample=N`
  - Phase 4: Evaluation
    - Runs quick evaluation games; computes a composite performance score.
  - Checkpointing
    - Initial checkpoint v0; regular checkpoints by interval; final checkpoint; optional retention cleanup.
# Heuristic‚ÄëGuided Bootstrapping for Chess DQN

Self‚Äëplay from scratch in chess is slow. To bootstrap the Q‚Äënetwork efficiently, we use a simple teacher policy (2‚Äì3‚Äëply minimax with a light evaluation) to generate diverse, high‚Äëquality data and pretrain by imitation before switching to DQN fine‚Äëtuning.

Teacher policy
- Search: 2‚Äì3‚Äëply minimax
- Evaluation: material values, piece‚Äësquare tables, mobility, king safety (normalized to a consistent scale)
- Outputs per state s:
  - a*: teacher‚Äôs best legal move (action index)
  - top‚Äëk(s): next best moves (e.g., k=3‚Äì5)
  - VŒ∏(s): scalar evaluation for side‚Äëto‚Äëmove, normalized to [-1, 1]

Temperature and diversity
- Softmax over teacher move scores with temperature œÑ: pT(a|s) ‚àù exp(score(a)/œÑ)
- Restrict sampling to top‚Äëk for quality + diversity
- œÑ in [0.5, 1.5] (anneal down during collection)

Phase A ‚Äî Data collection (teacher self‚Äëplay)
- Teacher vs teacher with temperature sampling; alternate colors, limit episode length
- Sample positions across the game; exclude trivial/no‚Äëprogress loops
- Prefer decisive games initially (filter or down‚Äëweight draws)
- Deduplicate by (FEN + side); cap occurrences per position; target 50k‚Äì200k unique positions
- Store per sample: state (or FEN), valid‚Äëaction mask, pT over valid actions, a*, VŒ∏(s), meta

Phase B ‚Äî Supervised imitation pretraining
- Primary loss (policy): cross‚Äëentropy between softmax(Q(s,¬∑)) masked to valid actions and pT(¬∑|s)
- Optional loss (value): MSE between value head (or Q(s,a*)) and VŒ∏(s) with small weight Œº
- Train with masked losses only over valid actions; standard split for train/val
- Metrics: Match@1/Match@k, KL(pT || pÃÇ), (optional) value MSE

Phase C ‚Äî DQN fine‚Äëtuning (optional shaping)
- Initialize Q‚Äënetwork from the pretrained weights (keep legal‚Äëaction masking and Double DQN)
- Optional potential‚Äëbased shaping using teacher eval (safe shaping): r‚Ä≤ = r + Œª[Œ≥Œ¶(s‚Ä≤) ‚àí Œ¶(s)], Œ¶(s)=VŒ∏(s)
- Optional small imitation bonus Œ¥ for picking a* or top‚Äëk(s)
- Anneal Œª and Œ¥ ‚Üí 0 over a few cycles to remove teacher bias

Evaluation
- Before/after imitation: Match@k on held‚Äëout set; greedy eval vs baseline
- During DQN: TD error trend, masked entropy on valid actions, win/draw/loss and average game length
- Vs teacher: agent vs greedy teacher for 50‚Äì100 games; target ~50%+ after bootstrap

Minimal implementation plan (works with current code)
1) Teacher (minimax+eval): implement 2‚Äì3‚Äëply move scoring and return a*, top‚Äëk, VŒ∏(s); add œÑ sampling over top‚Äëk.
2) Data collection mode: teacher self‚Äëplay to NDJSON/CSV; deduplicate by (FEN, side); filter draws; cap episodes and per‚Äëposition repeats.
3) Imitation pretraining: load dataset; compute valid‚Äëaction mask; train masked cross‚Äëentropy over valid actions; save checkpoint.
4) DQN warm‚Äëstart: add CLI to load pretrained model and run advanced training with legal‚Äëaction masking (and Double DQN enabled).
5) Verification: greedy eval (Œµ=0) vs baseline and vs teacher; confirm Match@k gain and reduced average game length.

Optional next (after basic bootstrap works)
- Potential‚Äëbased shaping (Œª annealed) and small match bonus Œ¥ (annealed)
- Prioritized replay and/or dueling architecture
- Deeper or alternative teachers (opening books, 3‚Äëply+ with pruning)

## Teacher Tools (implemented)

- Minimal 2‚Äì3 ply minimax teacher and NDJSON dataset generator are available under the chess engine module.
- Command examples:
  - Generate a small dataset (20 games, depth 2, top‚Äëk 5, œÑ=1.0):
    - `./gradlew :chess-engine:runTeacherCollector -Dargs="--collect --games 20 --depth 2 --topk 5 --tau 1.0 --out data/teacher.ndjson"`
  - Larger run with dedup and limited repeats per FEN:
    - `./gradlew :chess-engine:runTeacherCollector -Dargs="--collect --games 200 --max-plies 160 --depth 2 --topk 5 --tau 0.9 --max-repeats 2 --out data/teacher.ndjson"`

The NDJSON schema per line:
- `fen`: FEN string including side to move
- `side`: `"w"|"b"`
- `best_action`: action index in [0, 4095] via from√ó64+to mapping
- `top_k`: list of top‚Äëk action indices
- `teacher_policy`: map actionIndex‚Üíprobability over the top‚Äëk moves (softmax with œÑ)
- `value`: scalar evaluation in [-1,1] for side to move
- `valid_actions`: list of all legal action indices for the position
- `move`: algebraic long move (e.g., e2e4)
- `game_id`, `ply`, `ts`

You can use this dataset to run imitation pretraining (Phase B) by computing a masked cross‚Äëentropy loss between your model‚Äôs softmax over valid actions and `teacher_policy` for those same indices.

### Quick Start: Teacher Bootstrapping

- Collect a small teacher dataset (depth‚Äë2):
  - `./gradlew :chess-engine:runTeacherCollector -Dargs="--collect --games 50 --depth 2 --topk 5 --tau 1.0 --out data/teacher.ndjson"`

- Run imitation pretraining over the dataset and save a checkpoint (with optional label smoothing and train/val split). The default imitation architecture now uses three hidden layers `[512,256,128]` to match DQN defaults:
  - `./gradlew :chess-engine:runImitationTrainer -Dargs="--train-imitation --data data/teacher.ndjson --epochs 3 --batch 64 --lr 0.001 --smooth 0.05 --val-split 0.10 --out data/imitation_qnet.json"`

The imitation trainer prints per-epoch training loss and validation metrics, and finishes with a quick evaluation on a random subset. Metrics include:
- `match@1`: fraction where the model‚Äôs best valid action equals the teacher‚Äôs best action.
- `KL`: KL(pT || pÃÇ) between teacher policy and the model‚Äôs policy over valid actions.

### Expected Results (Teacher Learning)

When the pipeline is wired correctly, you should observe clear learning signals on teacher examples:
- `match@1` significantly above random: random top‚Äë1 is roughly 1/|valid| (typically <5%); after 1‚Äì3 epochs on a few thousand samples, expect 40‚Äì70% depending on depth, dataset size, and architecture.
- `KL` consistently decreasing across epochs (lower is better), indicating the model‚Äôs masked distribution aligns with the teacher.
- Qualitative: lower policy entropy on valid actions compared to uniform and more decisive move preferences in opening and tactical positions.

After imitation, warm‚Äëstart DQN fine‚Äëtuning. Two options:

- Use the profile that loads the imitation checkpoint automatically and tunes training for visibility/debugging:
  - `./gradlew :integration:runCli -Dargs="--train-advanced --cycles 3 --profile dqn_imitation_bootstrap"`
- Or pass the checkpoint path explicitly (overrides profile):
  - `./gradlew :integration:runCli -Dargs="--train-advanced --cycles 3 --profile dqn_imitation_bootstrap --load data/imitation_qnet.json"`

In evaluation matches (greedy), the warm‚Äëstarted agent should perform noticeably better than a reference untrained/baseline model. With a 2‚Äëply teacher dataset, expect stronger early‚Äëgame play and shorter average game lengths versus the reference model; with 3‚Äëply data these effects should be more pronounced. As DQN fine‚Äëtuning proceeds, the model can further improve beyond the teacher in frequently encountered positions.

### Profiles Overview

- `dqn_imitation_bootstrap`: loads `data/imitation_qnet.json`, sets `hiddenLayers: 512,256,128`, enables position rewards, `targetUpdateFrequency: 20`, and Double‚ÄëDQN. Use for faster visible progress when bootstrapping.
- `dqn_debug_masking`: enables Double‚ÄëDQN and a shorter target sync to surface masking/sync logs quickly.

Notes:
- Integration and chess‚Äëengine encoders are aligned at 839 inputs. Ensure the imitation model architecture (hidden layers) matches the DQN hidden layers to avoid shape mismatches when loading.
