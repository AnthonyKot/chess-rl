# Deep Q-Network (DQN) for Chess â€” Theory and Implementation Map

This note explains how canonical DQN works in theory, how we map it to the chess setting in this repo, and where the current implementation deviates from the ideal. Itâ€™s written to help you make informed decisions and to revisit details over time.

## 1) Conceptual Overview

Goal: Learn an action-value function Q(s, a) that estimates the expected discounted return when taking action a in state s and following the current policy thereafter.

Key components in â€œvanillaâ€ DQN:
- Neural network QÎ¸(s, a) approximates Q-values for all actions given a state.
- Target network QÎ¸Â¯(s, a) is a periodically updated copy of QÎ¸ to stabilize targets.
- Experience replay buffer stores (s, a, r, sâ€², done) tuples to decorrelate updates.
- Bellman target: y = r if done else r + Î³ max_{aâ€²} QÎ¸Â¯(sâ€², aâ€²)
- Loss: mean-squared-error between QÎ¸(s, a) and y (or Huber loss for robustness)
- Exploration: typically Îµ-gredy on Q-values.

## 2) Chess-specific mapping

- State space: 839 features per state (12 piece planes + side to move + castling rights + en passant oneâ€‘hot 64 + clocks). Aligned with chessâ€‘engine FeatureEncoding.
- Action space: 4096 indices (fromSquare Ã— toSquare), promotions handled via matching to legal moves
- Sparse/terminal rewards: +1/-1/0 on win/loss/draw; optional position-based shaping; optional step-limit penalty
- Valid action masking is crucial: Most of the 4096 actions are illegal in a given chess position; max over illegal actions corrupts targets.

## 3) Implementation Map (this repo)

Files to know (main pathnames abbreviated):
- rl-framework/RLAlgorithms.kt: DQNAlgorithm (Q-learning, target updates, replay), exploration strategies, metrics types
- integration/RealChessAgentFactory.kt: Builds real DQN agent (FeedforwardNetwork + Adam + Huber + replay), wires target frequency
- integration/ChessTrainingPipeline.kt: Wires next-state masking provider into the agent (agent.setNextActionProvider)
- integration/AdvancedSelfPlayTrainingPipeline.kt: Orchestrates self-play â†’ experience processing â†’ validated batch training â†’ evaluation, checkpoints, rollback
- integration/SelfPlaySystem.kt: (Now) concurrent self-play with per-game threads; produces EnhancedExperience
- integration/ChessEnvironment.kt: Encodes state, validates and executes moves, computes rewards, exposes valid actions
- nn-package/NeuralNetwork.kt: FeedforwardNetwork (layers, training loop, Adam, Huber), model save/load (JVM)

### 3.1 Q-network and target network
- Construction: RealChessAgentFactory.createRealDQNAgent builds two identical FeedforwardNetwork instances for Q and target.
- Optimizer/loss: Adam + HuberLoss(Î´=1.0) by default
- Synchronization cadence: targetUpdateFrequency updates (default 100, profile/CLI overridable)
- Copy mechanism: DQNAlgorithm.updateTargetNetwork() via SynchronizableNetwork.copyWeightsTo

### 3.2 Experience replay
- Experience buffers:
  - CircularExperienceBuffer (unseeded) and SeededCircularExperienceBuffer
  - DQNAlgorithm.updatePolicy(experiences) appends new tuples and samples a batch when size â‰¥ batchSize
- Bellman targets:
  - computeQTargets(batch): uses target network and (importantly) valid-action masking when provided (more below)

### 3.3 Action selection and exploration
- Exploration strategy: EpsilonGreedyStrategy (or seeded variant)
- selectAction: choose argmax Q(s, a) over valid actions with Îµ-greedy exploration
- Strict masking at adapter boundary: even if an internal policy proposes an illegal index, the adapter clamps selection to the best legal move (by prob or Q). This prevents illegal picks in training/eval.

### 3.4 Valid-action masking (critical for chess)
- Where itâ€™s set: ChessTrainingPipeline.init calls agent.setNextActionProvider(environment::getValidActions)
- DQNAlgorithm.computeQTargets:
  - If provider is set, it queries valid next actions for each sâ€² and computes max only over those actions.
  - One-time logs confirm masking is active:
    - â€œğŸ§© DQN next-state action masking enabled (provider set)â€
    - â€œğŸ§© DQN masking applied: valid next actions for sample=Nâ€
- Why it matters: prevents illegal-move Q-values from inflating targets and stalling learning.
- Caveat: environment.getValidActions(state) currently ignores the passed DoubleArray and uses the internal board. This is fine for on-policy calls in the same env, but for replay we want true per-state masking. See Â§6 for improvements.

### 3.5 Training loop in advanced pipeline
- Self-play (concurrent): SelfPlaySystem generates games between main and opponent agents; produces EnhancedExperience (with metadata and chess metrics)
- Experience processing: AdvancedExperienceManager filters/assesses quality; produces training batches
- Validated batch training: AdvancedSelfPlayTrainingPipeline.performValidatedBatchTraining
  - Batches are converted to basic experiences and passed to agent.trainBatch (which calls DQNAlgorithm.updatePolicy)
  - Metrics reported: loss, gradient norm (proxy), entropy; optional validation checks
- Target sync visibility: â€œğŸ” DQN target network synchronized at update=â€¦ (freq=â€¦)â€ shows up after the configured number of policy updates

### 3.6 Checkpointing and retention
- CheckpointManager creates checkpoints regularly and at end; model saved via FeedforwardNetwork.save
- Canonical best artifacts: `best_qnet.json` + `best_qnet_meta.json` + `best_checkpoint.txt` are updated when best advances.
- Retention policy: keep best, last N, and optionally every N-th; cleanup metadata-level by default (JVM file deletion could be added)
- Validation is existence-based and warns only; it never randomly gates loads.

## 4) Theoretical â€œidealâ€ vs current implementation

| Aspect | Ideal DQN | Current status |
| --- | --- | --- |
| Q-network | MLP with stable optimizer & loss | FeedforwardNetwork + Adam + Huber (OK) |
| Target network | Periodically synced | Implemented, freq configurable (OK) |
| Replay | IID batches from large buffer | Circular buffers (OK) |
| Masking | True per-state valid-action masking for sâ€² | Provider wired; env ignores passed state (needs improvement) |
| Exploration | Îµ-greedy | Implemented; warmup supported (OK) |
| Rewards | Terminal + optional shaping | Configurable; step-limit penalty supported (OK) |
| Concurrency | Parallel self-play | Thread pool per batch (JVM) (OK) |
| Metrics | True loss/entropy/grad stats | Reported from PolicyUpdateResult (OK) |
| Serialization | Save/load model | Implemented on JVM (OK) |

## 5) Practical guidance (DQN for chess)

- Keep masking enabled (default) and verify logs once per run.
- Use shorter games (e.g., 80â€“100 max steps) and a small negative step-limit penalty to avoid flat random games.
- Consider reward shaping early (enablePositionRewards=true) to provide signal when decisive results are rare.
- Target update frequency 50â€“200 is typical; use 20 for quick debug.
- Check loss, entropy, gradient-norm and self-play win/draw rates: expect modest trends, not dramatic changes in a few cycles.
- Concurrency: set maxConcurrentGames near your core count; we synchronize agent calls internally to ensure thread safety.

## 5.1) â€œAdvancedâ€ Training (what the CLI actually does)

Command example
```
./gradlew :integration:runCli -Dargs="--train-advanced --cycles 3 --profile dqn_debug_masking --seed 12345"
```

What happens (one endâ€‘toâ€‘end cycle)
- Phase 1 â€” Selfâ€‘play generation
  - Runs N concurrent selfâ€‘play games (thread pool) until the requested total is reached.
  - Produces EnhancedExperience per move with chess metadata and a base quality score.
  - Progress lines show draw/win rates, average game length, and experiences collected.
- Phase 2 â€” Experience processing
  - Quality assessment and filtering; buffers updated (main/highâ€‘quality/recent) with simple retention rules.
- Phase 3 â€” Batch training with validation
  - Samples experiences into batches and calls `agent.trainBatch` (DQNAlgorithm.updatePolicy) per batch.
  - Prints perâ€‘batch metrics from the algorithm result (loss, policy entropy, gradient norm; optional EMA and extras).
  - Periodically prints target sync: `ğŸ” DQN target network synchronized at update=â€¦ (freq=â€¦)`.
  - Phase 4 â€” Evaluation (internal)
    - Plays a small number of evaluation games to estimate reward/win/draw/loss and outcomeScore = (wins + 0.5Â·draws)/games.
    - Peer match vs previous best each cycle: current vs best headâ€‘toâ€‘head; promote on tie or win (no Elo).
    - Masking/legality: evaluation and H2H enforce strict legal moves. `invalid_moves` should be 0; any >0 indicates an encoder/mapping issue to fix before scaling.
- Checkpointing
  - Creates an initial checkpoint (v0), regular checkpoints every `checkpointInterval` cycles, and a final checkpoint.
  - Optional retention cleanup at the end (keep best + last K + every Nth) per config/profile.

Why you sometimes see â€œCycle 4/3â€
- The advanced pipeline treats `--cycles K` as K + 1 iterations with an initial warmup cycle in some configurations. Thatâ€™s why a 3â€‘cycle run can display â€œAdvanced Training Cycle 4/3â€.

Notes on flags/profiles
- Profiles (profiles.yaml) set defaults for: algorithm, max steps, concurrency, reward shaping, warmups, checkpoint cadence, retention, target sync frequency, and draw reporting for stepâ€‘limit.
- CLI flags override profile keys: `--max-steps`, `--concurrency`, `--target-update`, `--checkpoint-interval`, retention flags, etc.
- Masking verification: once per run you should see
  - `ğŸ§© DQN next-state action masking enabled (provider set)`
  - `ğŸ§© DQN masking applied: valid next actions for sample=N`

About `--double-dqn`
- Doubleâ€‘DQN changes the target to use the online net to select the next action and the target net to evaluate it. Supported here; enable via profile (e.g., `dqn_debug_masking`, `dqn_imitation_bootstrap`) or `--double-dqn`.

## 5.2) How to verify that something was learned

From logs (first few cycles)
- Target syncs: You should see `ğŸ” â€¦ synchronized â€¦` at the configured frequency (e.g., every 50 updates). This confirms updates are flowing.
- Entropy < 8.29: Policy entropy near ln(4096) â‰ˆ 8.29 means uniform. Seeing values lower than that (e.g., ~3.0) indicates nonâ€‘uniform preferences emerging. Beware: too low too fast can signal collapse; modest reduction is good.
- Loss variability: Exact scale depends on network/loss; look for nonâ€‘flat behavior and mild downward drift over cycles.
- Selfâ€‘play stats: With short episodes and shaping, expect many stepâ€‘limit draws early. As the policy improves, evaluate vs the baseline heuristic to see improvements independent of selfâ€‘play reporting.

Crossâ€‘cycle checks (recommended)
- Baseline evaluation: Run `--eval-baseline --games 20 --load-best` after a few cycles. Compare win/draw/loss to earlier runs.
- Experience buffer quality: Use the analyzer to check reward variance > 0, terminal ratio > 0, and reasonable action diversity.
- Checkpoint diffs: Ensure new checkpoints are being created and the â€œbest modelâ€ version eventually advances beyond 0.

If you see 100% draws
- With `maxStepsPerGame=80..100` and `treatStepLimitAsDraw=true`, many early games will report as draws by step limit. Thatâ€™s OK â€” verify that
  - Masking logs appear,
  - Batch metrics vary,
  - Baseline evaluation slowly improves.

## 5.3) Metric glossary (as printed)
- loss: The batch loss (Huber/MSE proxy for TD error) aggregated over samples.
- gradient norm: A proxy norm at the network output (or true norm when available) for stability checks.
- policy entropy: Entropy computed from softmax over Q-values or policy logits; lower implies more certainty.
- td (if printed): An estimate of TD error magnitude (implementationâ€‘dependent; not always printed).

## 6) Known gaps and next steps

1) True per-state masking for replay
- Issue: environment.getValidActions(state: DoubleArray) uses internal board; for replayed (sâ€²) it should compute from the vector.
- Options:
  - Decode function: state â†’ board (inverse of encoder) and compute legal moves on the synthetic board.
  - Per-experience mask: store valid-action masks alongside experiences and extend DQNAlgorithm to consume masks in targets.

2) Metrics & monitoring
- Replace simulated performance monitoring with real timings around: NN forward/backward/train, replay sampling, move generation.

3) Compression
- Checkpoints use .json.gz extension but content is JSON. Add actual gzip for JVM saves/loads if desired.

4) Opponent strategy
- Profiles currently copy main weights or use simple strategies. More varied opponents can speed training robustness.

5) Doubleâ€‘DQN
- Optionally add a Doubleâ€‘DQN mode (profile/flag) that selects the argmax action with the online Q net and evaluates it with the target net in the target computation.

## 7) Pseudocode (reference)

Canonical DQN (with masking and target net):
```
Initialize QÎ¸ and target QÎ¸Â¯; replay buffer D
for each step do
  Observe s, select a=Îµ-greedy(QÎ¸(s,Â·)), execute, get r, sâ€², done
  D.add(s,a,r,sâ€²,done)
  if |D| â‰¥ batchSize:
    B â† sample minibatch from D
    for (s,a,r,sâ€²,done) in B:
      if done: y â† r
      else:
        A_valid â† valid_actions(sâ€²)
        y â† r + Î³ max_{aâ€²âˆˆA_valid} QÎ¸Â¯(sâ€²,aâ€²)
      L â† Huber(QÎ¸(s,a), y)  // or MSE
    Î¸ â† Î¸ - Î± âˆ‡Î¸ L
  Every N updates: QÎ¸Â¯ â† QÎ¸  // target sync
```

In our pipeline:
- SelfPlaySystem generates Batches of EnhancedExperience via parallel games
- AdvancedExperienceManager processes/filters into minibatches
- performValidatedBatchTraining calls agent.trainBatch (â†’ DQNAlgorithm.updatePolicy)
- Masking provider gives valid actions for sâ€² (with the caveat described above)
- Target sync prints a log when triggered

## 8) Quick glossary
- Q-network: Approximator for action-values Q(s,a)
- Target network: Fixed-lag copy of Q-network for stable targets
- Replay buffer: Storage for experience tuples, sampled IID for updates
- Masking: Restrict backups to valid actions, crucial in large, constrained action spaces
- Îµ-greedy: Choose best action w.p. 1-Îµ else random valid action
- Huber loss: Robust loss less sensitive to outliers than MSE

---
If you want, I can prototype â€œper-stateâ€ masking via masks embedded in experiences or build the inverse encoder to decode state arrays to ChessBoard for exact legality checks in replay.
## Functional Summary (step-by-step)

- Command: `./gradlew :integration:runCli -Dargs="--train-advanced --cycles 3 --profile dqn_debug_masking --seed 12345"`
- Effects by phase:
  - Phase 1: Self-play generation
    - Runs 20 games with 8 concurrent workers (thread pool).
    - Collects EnhancedExperience tuples; reports progress (win/draw, avg length).
  - Phase 2: Experience processing
    - Computes quality, filters, fills buffers.
  - Phase 3: Batch training with validation
    - Samples experiences into batches; calls `DQNAlgorithm.updatePolicy` (via `agent.trainBatch`) per batch.
    - Prints per-batch metrics: loss, gradient norm (and EMA), policy entropy (and EMA).
    - Prints target sync when frequency is hit: `ğŸ” DQN target network synchronized at update=40 (freq=20)`.
    - Masking logs (printed once early) confirm valid-action masking for next-state targets:
      - `ğŸ§© DQN next-state action masking enabled (provider set)`
      - `ğŸ§© DQN masking applied: valid next actions for sample=N`
  - Phase 4: Evaluation
    - Runs quick evaluation games; reports outcome metrics (win/draw/loss, average length) and computes outcomeScore = (wins + 0.5Â·draws)/games.
  - Checkpointing
    - Initial checkpoint v0; regular checkpoints by interval; final checkpoint; optional retention cleanup.
# Heuristicâ€‘Guided Bootstrapping for Chess DQN

Selfâ€‘play from scratch in chess is slow. To bootstrap the Qâ€‘network efficiently, we use a simple teacher policy (2â€“3â€‘ply minimax with a light evaluation) to generate diverse, highâ€‘quality data and pretrain by imitation before switching to DQN fineâ€‘tuning.

Teacher policy
- Search: 2â€“3â€‘ply minimax
- Evaluation: material values, pieceâ€‘square tables, mobility, king safety (normalized to a consistent scale)
- Outputs per state s:
  - a*: teacherâ€™s best legal move (action index)
  - topâ€‘k(s): next best moves (e.g., k=3â€“5)
  - VÎ¸(s): scalar evaluation for sideâ€‘toâ€‘move, normalized to [-1, 1]

Temperature and diversity
- Softmax over teacher move scores with temperature Ï„: pT(a|s) âˆ exp(score(a)/Ï„)
- Restrict sampling to topâ€‘k for quality + diversity
- Ï„ in [0.5, 1.5] (anneal down during collection)

Phase A â€” Data collection (teacher selfâ€‘play)
- Teacher vs teacher with temperature sampling; alternate colors, limit episode length
- Sample positions across the game; exclude trivial/noâ€‘progress loops
- Prefer decisive games initially (filter or downâ€‘weight draws)
- Deduplicate by (FEN + side); cap occurrences per position; target 50kâ€“200k unique positions
- Store per sample: state (or FEN), validâ€‘action mask, pT over valid actions, a*, VÎ¸(s), meta

Phase B â€” Supervised imitation pretraining
- Primary loss (policy): crossâ€‘entropy between softmax(Q(s,Â·)) masked to valid actions and pT(Â·|s)
- Optional loss (value): MSE between value head (or Q(s,a*)) and VÎ¸(s) with small weight Î¼
- Train with masked losses only over valid actions; standard split for train/val
- Metrics: Match@1/Match@k, KL(pT || pÌ‚), (optional) value MSE

Phase C â€” DQN fineâ€‘tuning (optional shaping)
- Initialize Qâ€‘network from the pretrained weights (keep legalâ€‘action masking and Double DQN)
- Optional potentialâ€‘based shaping using teacher eval (safe shaping): râ€² = r + Î»[Î³Î¦(sâ€²) âˆ’ Î¦(s)], Î¦(s)=VÎ¸(s)
- Optional small imitation bonus Î´ for picking a* or topâ€‘k(s)
- Anneal Î» and Î´ â†’ 0 over a few cycles to remove teacher bias

Evaluation
- Before/after imitation: Match@k on heldâ€‘out set; greedy eval vs baseline
- During DQN: TD error trend, masked entropy on valid actions, win/draw/loss and average game length
- Vs teacher: agent vs greedy teacher for 50â€“100 games; target ~50%+ after bootstrap

Minimal implementation plan (works with current code)
1) Teacher (minimax+eval): implement 2â€“3â€‘ply move scoring and return a*, topâ€‘k, VÎ¸(s); add Ï„ sampling over topâ€‘k.
2) Data collection mode: teacher selfâ€‘play to NDJSON/CSV; deduplicate by (FEN, side); filter draws; cap episodes and perâ€‘position repeats.
3) Imitation pretraining: load dataset; compute validâ€‘action mask; train masked crossâ€‘entropy over valid actions; save checkpoint.
4) DQN warmâ€‘start: add CLI to load pretrained model and run advanced training with legalâ€‘action masking (and Double DQN enabled).
5) Verification: greedy eval (Îµ=0) vs baseline and vs teacher; confirm Match@k gain and reduced average game length.

Optional next (after basic bootstrap works)
- Potentialâ€‘based shaping (Î» annealed) and small match bonus Î´ (annealed)
- Prioritized replay and/or dueling architecture
- Deeper or alternative teachers (opening books, 3â€‘ply+ with pruning)

## Teacher Tools (implemented)

- Minimal 2â€“3 ply minimax teacher and NDJSON dataset generator are available under the chess engine module.
- Command examples:
  - Generate a small dataset (20 games, depth 2, topâ€‘k 5, Ï„=1.0):
    - `./gradlew :chess-engine:runTeacherCollector -Dargs="--collect --games 20 --depth 2 --topk 5 --tau 1.0 --out data/teacher.ndjson"`
  - Larger run with dedup and limited repeats per FEN:
    - `./gradlew :chess-engine:runTeacherCollector -Dargs="--collect --games 200 --max-plies 160 --depth 2 --topk 5 --tau 0.9 --max-repeats 2 --out data/teacher.ndjson"`

The NDJSON schema per line:
- `fen`: FEN string including side to move
- `side`: `"w"|"b"`
- `best_action`: action index in [0, 4095] via fromÃ—64+to mapping
- `top_k`: list of topâ€‘k action indices
- `teacher_policy`: map actionIndexâ†’probability over the topâ€‘k moves (softmax with Ï„)
- `value`: scalar evaluation in [-1,1] for side to move
- `valid_actions`: list of all legal action indices for the position
- `move`: algebraic long move (e.g., e2e4)
- `game_id`, `ply`, `ts`

You can use this dataset to run imitation pretraining (Phase B) by computing a masked crossâ€‘entropy loss between your modelâ€™s softmax over valid actions and `teacher_policy` for those same indices.

### Quick Start: Teacher Bootstrapping

- Collect a small teacher dataset (depthâ€‘2):
  - `./gradlew :chess-engine:runTeacherCollector -Dargs="--collect --games 50 --depth 2 --topk 5 --tau 1.0 --out data/teacher.ndjson"`

- Run imitation pretraining over the dataset and save a checkpoint (with optional label smoothing and train/val split). The default imitation architecture now uses three hidden layers `[512,256,128]` to match DQN defaults:
  - `./gradlew :chess-engine:runImitationTrainer -Dargs="--train-imitation --data data/teacher.ndjson --epochs 3 --batch 64 --lr 0.001 --smooth 0.05 --val-split 0.10 --out data/imitation_qnet.json"`

The imitation trainer prints per-epoch training loss and validation metrics, and finishes with a quick evaluation on a random subset. Metrics include:
- `match@1`: fraction where the modelâ€™s best valid action equals the teacherâ€™s best action.
- `KL`: KL(pT || pÌ‚) between teacher policy and the modelâ€™s policy over valid actions.

### Expected Results (Teacher Learning)

When the pipeline is wired correctly, you should observe clear learning signals on teacher examples:
- `match@1` significantly above random: random topâ€‘1 is roughly 1/|valid| (typically <5%); after 1â€“3 epochs on a few thousand samples, expect 40â€“70% depending on depth, dataset size, and architecture.
- `KL` consistently decreasing across epochs (lower is better), indicating the modelâ€™s masked distribution aligns with the teacher.
- Qualitative: lower policy entropy on valid actions compared to uniform and more decisive move preferences in opening and tactical positions.

After imitation, warmâ€‘start DQN fineâ€‘tuning. Two options:

- Use the profile that loads the imitation checkpoint automatically and tunes training for visibility/debugging:
  - `./gradlew :integration:runCli -Dargs="--train-advanced --cycles 3 --profile dqn_imitation_bootstrap"`
- Or pass the checkpoint path explicitly (overrides profile):
  - `./gradlew :integration:runCli -Dargs="--train-advanced --cycles 3 --profile dqn_imitation_bootstrap --load data/imitation_qnet.json"`

In evaluation matches (greedy), the warmâ€‘started agent should perform noticeably better than a reference untrained/baseline model. With a 2â€‘ply teacher dataset, expect stronger earlyâ€‘game play and shorter average game lengths versus the reference model; with 3â€‘ply data these effects should be more pronounced. As DQN fineâ€‘tuning proceeds, the model can further improve beyond the teacher in frequently encountered positions.

### Profiles Overview

- `dqn_imitation_bootstrap`: loads `data/imitation_qnet.json`, sets `hiddenLayers: 512,256,128`, enables position rewards, `targetUpdateFrequency: 20`, and Doubleâ€‘DQN. Use for faster visible progress when bootstrapping.
- `dqn_debug_masking`: enables Doubleâ€‘DQN and a shorter target sync to surface masking/sync logs quickly.

Notes:
- Integration and chessâ€‘engine encoders are aligned at 839 inputs. Ensure the imitation model architecture (hidden layers) matches the DQN hidden layers to avoid shape mismatches when loading.
