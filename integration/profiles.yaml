# Chess RL Essential Training Profiles
#
# This file replaces the complex profiles.yaml with 3 focused profiles
# that contain only essential parameters for training competitive chess agents.
#
# Based on CONFIGURATION_AUDIT.md analysis, experimental parameters have been removed
# and only the 18 essential parameters that impact training effectiveness are included.

profiles:
  # Fast Debug Profile
  # Optimized for rapid development iteration and testing
  fast-debug:
    # Self-Play Configuration (reduced for speed)
    gamesPerCycle: 5
    maxCycles: 10
    maxConcurrentGames: 2
    maxStepsPerGame: 40
    
    # Neural Network (smaller for faster training)
    hiddenLayers: [256, 128]
    learningRate: 0.001
    batchSize: 32
    
    # RL Training (standard settings)
    explorationRate: 0.1
    targetUpdateFrequency: 100
    maxExperienceBuffer: 10000
    
    # Rewards (standard structure)
    winReward: 1.0
    lossReward: -1.0
    drawReward: -0.2
    stepLimitPenalty: -1.0
    
    # System (frequent checkpoints for debugging)
    checkpointInterval: 2
    checkpointDirectory: "checkpoints/fast-debug"
    evaluationGames: 20

  # Long Train Profile  
  # Optimized for training competitive agents in production
  long-train:
    # Self-Play Configuration (increased for thorough training)
    gamesPerCycle: 50
    maxCycles: 200
    maxConcurrentGames: 8
    maxStepsPerGame: 120
    
    # Neural Network (larger for better capacity)
    hiddenLayers: [768, 512, 256]
    learningRate: 0.0005  # Lower for stability with larger network
    batchSize: 64
    
    # RL Training (optimized for larger network)
    explorationRate: 0.15
    targetUpdateFrequency: 100
    maxExperienceBuffer: 50000
    
    # Rewards (anti-draw incentive for competitive play)
    winReward: 1.0
    lossReward: -1.0
    drawReward: -0.2
    stepLimitPenalty: -1.0
    
    # System (standard checkpointing)
    checkpointInterval: 5
    checkpointDirectory: "checkpoints/long-train"
    evaluationGames: 100

  # Evaluation Only Profile
  # Optimized for consistent, reproducible evaluation runs
  eval-only:
    # Self-Play Configuration (not used for evaluation)
    gamesPerCycle: 20
    maxCycles: 1
    maxConcurrentGames: 1  # Single-threaded for consistency
    maxStepsPerGame: 80
    
    # Neural Network (standard settings)
    hiddenLayers: [512, 256, 128]
    learningRate: 0.001
    batchSize: 64
    
    # RL Training (standard settings)
    explorationRate: 0.1
    targetUpdateFrequency: 100
    maxExperienceBuffer: 50000
    
    # Rewards (standard structure)
    winReward: 1.0
    lossReward: -1.0
    drawReward: -0.2
    stepLimitPenalty: -1.0
    
    # System (deterministic evaluation)
    seed: 12345  # Fixed seed for reproducible results
    checkpointInterval: 1
    checkpointDirectory: "checkpoints/eval-only"
    evaluationGames: 500  # Many games for statistical significance

# Profile Usage Examples:
#
# Fast development iteration:
#   --profile fast-debug
#
# Production training:
#   --profile long-train --seed 12345
#
# Evaluation runs:
#   --profile eval-only --evaluation-games 1000
#
# Custom configuration:
#   --profile long-train --games-per-cycle 30 --learning-rate 0.002