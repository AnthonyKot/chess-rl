# Chess RL Essential Training Profiles
#
# This file replaces the complex profiles.yaml with 3 focused profiles
# that contain only essential parameters for training competitive chess agents.
#
# Based on CONFIGURATION_AUDIT.md analysis, experimental parameters have been removed
# and only the essential parameters that impact training effectiveness are included.

profiles:
  # Fast Debug Profile
  # Optimized for rapid development iteration and testing
  fast-debug:
    # Self-Play Configuration (reduced for speed)
    gamesPerCycle: 10
    maxCycles: 10
    maxConcurrentGames: 2
    maxStepsPerGame: 60
    
    # Neural Network (smaller for faster training)
    hiddenLayers: [256, 128]
    learningRate: 0.001
    batchSize: 32
    
    # RL Training (optimized for learning)
    explorationRate: 0.08
    initialExplorationRate: 0.35
    explorationDecaySteps: 600
    targetUpdateFrequency: 150
    doubleDqn: true
    maxExperienceBuffer: 10000
    
    # Rewards (optimized for learning)
    winReward: 1.0
    lossReward: -1.0
    drawReward: 0.0
    stepLimitPenalty: -0.5
    
    # System (frequent checkpoints for debugging)
    checkpointInterval: 2
    checkpointDirectory: "checkpoints/fast-debug"
    evaluationGames: 20

  # Long Train Profile  
  # Optimized for training competitive agents in production
  long-train:
    # Self-Play Configuration (increased for thorough training)
    gamesPerCycle: 50
    maxCycles: 200
    maxConcurrentGames: 8
    maxStepsPerGame: 150
    
    # Neural Network (larger for better capacity)
    hiddenLayers: [768, 512, 256]
    learningRate: 0.0003  # Lower for stability with larger network
    batchSize: 64
    
    # RL Training (optimized for stable learning)
    explorationRate: 0.03
    initialExplorationRate: 0.4
    explorationDecaySteps: 2000
    targetUpdateFrequency: 300
    doubleDqn: true
    maxExperienceBuffer: 100000
    
    # Rewards (balanced for competitive play)
    winReward: 1.0
    lossReward: -1.0
    drawReward: -0.05
    stepLimitPenalty: -0.6
    
    # System (standard checkpointing)
    checkpointInterval: 5
    checkpointDirectory: "checkpoints/long-train"
    evaluationGames: 20
    trainOpponentType: minimax-softmax
    trainOpponentDepth: 1
    trainOpponentSoftmaxTemperature: 2.0

  # Long Train (Mixed Opponents)
  # Uses a blend of heuristic and shallow minimax opponents during training
  long-train-mixed:
    gamesPerCycle: 50
    maxCycles: 200
    maxConcurrentGames: 8
    maxStepsPerGame: 150

    hiddenLayers: [768, 512, 256]
    learningRate: 0.0003
    batchSize: 64

    explorationRate: 0.03
    initialExplorationRate: 0.4
    explorationDecaySteps: 2000
    targetUpdateFrequency: 300
    doubleDqn: true
    maxExperienceBuffer: 100000
    replayType: PRIORITIZED

    winReward: 1.0
    lossReward: -1.0
    drawReward: -0.05
    stepLimitPenalty: -0.6

    checkpointInterval: 5
    checkpointDirectory: "checkpoints/long-train-mixed"
    evaluationGames: 20
    trainOpponentType: random
    trainOpponentDepth: 2

  # Evaluation Only Profile
  # Optimized for consistent, reproducible evaluation runs
  eval-only:
    # Self-Play Configuration (not used for evaluation)
    gamesPerCycle: 20
    maxCycles: 1
    maxConcurrentGames: 1  # Single-threaded for consistency
    maxStepsPerGame: 120
    
    # Neural Network (standard settings)
    hiddenLayers: [512, 256, 128]
    learningRate: 0.0005
    batchSize: 64
    
    # RL Training (match training exploration so evaluation stays comparable)
    explorationRate: 0.05
    targetUpdateFrequency: 200
    doubleDqn: true
    maxExperienceBuffer: 50000
    
    # Rewards (balanced structure)
    winReward: 1.0
    lossReward: -1.0
    drawReward: 0.0
    stepLimitPenalty: -0.5
    
    # System (deterministic evaluation)
    seed: 12345  # Fixed seed for reproducible results
    checkpointInterval: 1
    checkpointDirectory: "checkpoints/eval-only"
    evaluationGames: 500  # Many games for statistical significance

# Profile Usage Examples:
#
# Fast development iteration (optimized for learning):
#   --profile fast-debug
#
# Production training (self-play only):
#   --profile long-train --seed 12345
# Mixed curriculum (heuristic + minimax opponents):
#   --profile long-train-mixed --train-opponent random
#
# Evaluation runs (no exploration, deterministic):
#   --profile eval-only --evaluation-games 1000
#
# Custom configuration:
#   --profile long-train --games-per-cycle 30 --learning-rate 0.0002
#
# Key Improvements in These Profiles:
# - Neutral draw reward (0.0) to allow natural play
# - Lower exploration rates for more exploitation of learned knowledge  
# - Double DQN enabled for better value estimation
# - Longer games allowed (60-150 steps) for better learning
# - More conservative learning rates for stability
