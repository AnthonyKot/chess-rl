# Optimizer State
optimizer_type=adam
learning_rate=0.1
beta1=0.9
beta2=0.999
epsilon=1.0E-8
momentum=0.0
# Note: Internal optimizer state (momentum/velocity buffers)
# is not saved in this simplified implementation.
# The optimizer will be recreated with fresh internal state.
